# Stable-Diffusion-Model-Serving
This repository is a comprehensive guide for serving generative models, specifically the Stable Diffusion models.

## TODO
- [ ] Model Inference Example
- [ ] Export model to various format(onnx, tensorrt) example
- [ ] Triton Inference Server Example
- [ ] Test Model latency
- [ ] Configure the API server and the model server (Triton Inference Server based).
- [ ] Dockerize
- [ ] Loadtest
- [ ] Create lightweight model
- [ ] Model acceration 
